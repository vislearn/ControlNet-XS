model:
  base_learning_rate: 1.0e-05
  target: sgm.models.diffusion.ControlledDiffusionEngine
  params:
    scale_factor: 0.13025
    disable_first_stage_autocast: true
    input_key : "image"
    denoiser_config:
      target: sgm.modules.diffusionmodules.denoiser.ControlledDiscreteDenoiser
      params:
        num_idx: 1000
        weighting_config:
          target: sgm.modules.diffusionmodules.denoiser_weighting.EpsWeighting
        scaling_config:
          target: sgm.modules.diffusionmodules.denoiser_scaling.EpsScaling
        discretization_config:
          target: sgm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization
    sd_locked: true
    skip_wrapper: true
    ckpt_path: /home/dell/workspace/models/sd_xl_base_1.0_0.9vae.safetensors   #  path to the StableDiffusion-XL weights
    # ckpt_path_control: ./checkpoints/sdxl_encD_depth_48m.safetensors        #  path to the ControlNet-XS weights
    ckpt_path_control: /home/dell/workspace/models/sdxl_encD_depth_48m.safetensors #  path to the ControlNet-XS weights
    network_config:
      target: sgm.modules.diffusionmodules.twoStreamControl.TwoStreamControlNet
      params:
        adm_in_channels: 2816
        num_classes: sequential
        use_checkpoint: true
        in_channels: 4
        out_channels: 4
        hint_channels: 3
        model_channels: 320
        attention_resolutions:
        - 4
        - 2
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 4
        num_head_channels: 64
        use_spatial_transformer: true
        use_linear_in_transformer: true
        transformer_depth:
        - 1
        - 2
        - 10
        context_dim: 2048
        spatial_transformer_attn_type: softmax-xformers
        legacy: false
        infusion2control: cat
        guiding: encoder_double
        two_stream_mode: cross
        control_model_ratio: 0.1
        control_mode: midas
        learn_embedding: true
    conditioner_config:
      target: sgm.modules.encoders.modules.GeneralConditioner
      params:
        emb_models:
        - is_trainable: false
          # input_key: caption
          input_key:  txt
          target: sgm.modules.encoders.modules.FrozenCLIPEmbedder
          params:
            layer: hidden
            layer_idx: 11
        - is_trainable: false
          # input_key: caption
          input_key:  txt
          target: sgm.modules.encoders.modules.FrozenOpenCLIPEmbedder2
          params:
            arch: ViT-bigG-14
            version: laion2b_s39b_b160k
            freeze: true
            layer: penultimate
            always_return_pooled: true
            legacy: false
        - is_trainable: false
          input_key: original_size_as_tuple # bad ---------TODO
          target: sgm.modules.encoders.modules.ConcatTimestepEmbedderND
          params:
            outdim: 256
        - is_trainable: false
          input_key: crop_coords_top_left
          target: sgm.modules.encoders.modules.ConcatTimestepEmbedderND
          params:
            outdim: 256
        - is_trainable: false
          input_key: target_size_as_tuple
          target: sgm.modules.encoders.modules.ConcatTimestepEmbedderND
          params:
            outdim: 256
    first_stage_config:
      target: sgm.models.autoencoder.AutoencoderKLInferenceWrapper
      params:
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          attn_type: vanilla-xformers
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    loss_fn_config:
      target: sgm.modules.diffusionmodules.loss.StandardDiffusionLoss
      params:
        batch2model_keys:
        - hint
        sigma_sampler_config:
          target: sgm.modules.diffusionmodules.sigma_sampling.DiscreteSampling
          params:
            num_idx: 1000
            discretization_config:
              target: sgm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization
    sampler_config:
      target: sgm.modules.diffusionmodules.sampling.EulerEDMSampler
      params:
        num_steps: 50
        discretization_config:
          target: sgm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization
        guider_config:
          target: sgm.modules.diffusionmodules.guiders.VanillaCFG
          params:
            scale: 7.5

# data:
#   target: sgm.data.dataset.StableDataModuleFromConfig
#   params:
#     train:
#       datapipeline:
#         urls:
#           # USER: adapt this path the root of your custom dataset
#           # - "DATA_PATH"
#           # dataset_name='/home/dell/workspace/dataset/laion_data/',
#           # dataset_name='/home/dell/workspace/controlnet/sampleset/',
#           # dataset_name='/home/dell/workspace/T2IAdapter-SDXL/face3w/face3w.py',
#           /home/dell/workspace/T2IAdapter-SDXL/face3w/face1set.py
#         pipeline_config:
#           shardshuffle: 10000
#           sample_shuffle: 10000


#         decoders:
#           - "pil"

#         postprocessors:
#           - target: sdata.mappers.TorchVisionImageTransforms
#             params:
#               key: 'jpg' # USER: you might wanna adapt this for your custom dataset
#               transforms:
#                 - target: torchvision.transforms.Resize
#                   params:
#                     size: 256
#                     interpolation: 3
#                 - target: torchvision.transforms.ToTensor
#           - target: sdata.mappers.Rescaler
#             # USER: you might wanna use non-default parameters due to your custom dataset
#           - target: sdata.mappers.AddOriginalImageSizeAsTupleAndCropToSquare
#             # USER: you might wanna use non-default parameters due to your custom dataset

#       loader:
#         batch_size: 2
#         num_workers: 1

data:
  target: data.datam.DataModuleFromConfig
  params:
    batch_size: 2
    num_workers: 1
    num_val_workers: 0 # Avoid a weird val dataloader issue
    train:
      # target: ldm.data.simple.hf_dataset
      target: data.simple.hf_dataset
      params:
        # name: lambdalabs/pokemon-blip-captions
        name: /home/dell/workspace/dataset/lambdalabs___pokemon-blip-captions/
        image_transforms:
        - target: torchvision.transforms.Resize
          params:
            size: 512
            interpolation: 3
        - target: torchvision.transforms.RandomCrop
          params:
            size: 512
        - target: torchvision.transforms.RandomHorizontalFlip
    validation:
      target: data.simple.TextOnly
      params:
        captions:
        - "A pokemon with green eyes, large wings, and a hat"
        - "A cute bunny rabbit"
        - "Yoda"
        - "An epic landscape photo of a mountain"
        output_size: 512
        n_gpus: 1 # small hack to make sure we see all our samples

    # loader:
    #   batch_size: 2
    #   num_workers: 1

lightning:
  modelcheckpoint:
    params:
      # every_n_train_steps: 30
      every_n_train_steps: 300

  callbacks:
    metrics_over_trainsteps_checkpoint:
      params:
        # every_n_train_steps: 25
        every_n_train_steps: 250

    image_logger:
      target: main.ImageLogger
      params:
        disabled: False
        enable_autocast: False
        batch_frequency: 200
        max_images: 2
        increase_log_steps: True
        log_first_step: False
        log_images_kwargs:
          use_ema_scope: False
          N: 8
          n_rows: 2

  trainer:
    devices: 0,
    benchmark: True
    num_sanity_val_steps: 0
    accumulate_grad_batches: 1
    max_epochs: 3
    
